"""
Script principal du pipeline ETL
Orchestre t√©l√©chargement ‚Üí exploration ‚Üí nettoyage
"""

import sys
import logging
from pathlib import Path
from datetime import datetime

# Ajouter le r√©pertoire src au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from pipeline.download_data import download_all_datasets
from pipeline.explore_and_clean import main as explore_and_clean

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def run_pipeline(force_download: bool = False, skip_download: bool = False) -> bool:
    """
    Ex√©cute le pipeline ETL complet
    
    Args:
        force_download: Force le t√©l√©chargement m√™me si fichiers existent
        skip_download: Ignore l'√©tape de t√©l√©chargement
    
    Returns:
        True si succ√®s
    """
    
    logger.info("\n" + "=" * 80)
    logger.info("üöÄ D√âMARRAGE PIPELINE ETL - ACCIDENTS ROUTIERS")
    logger.info("=" * 80)
    logger.info(f"Heure: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # √âtape 1: T√©l√©chargement
        if not skip_download:
            logger.info("\nüì• √âTAPE 1: T√âL√âCHARGEMENT")
            logger.info("-" * 80)
            
            results = download_all_datasets(force=force_download)
            
            # V√©rifier si au moins un dataset a r√©ussi
            success_count = sum(1 for r in results.values() if r.get("success"))
            if success_count == 0:
                logger.error("‚úó Aucun t√©l√©chargement r√©ussi")
                return False
            
            logger.info(f"‚úì {success_count}/{len(results)} datasets t√©l√©charg√©s")
        else:
            logger.info("\n‚è≠Ô∏è  T√©l√©chargement ignor√© (--skip-download)")
        
        # √âtape 2: Exploration et nettoyage
        logger.info("\nüîç √âTAPE 2: EXPLORATION ET NETTOYAGE")
        logger.info("-" * 80)
        
        success = explore_and_clean()
        
        if not success:
            logger.error("‚úó Erreur lors de l'exploration/nettoyage")
            return False
        
        # Succ√®s
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ PIPELINE COMPL√âT√â AVEC SUCC√àS")
        logger.info("=" * 80)
        logger.info("\nüìÇ Donn√©es nettoy√©es disponibles dans: data/clean/")
        logger.info("üìã Logs disponibles dans: pipeline.log")
        
        return True
        
    except Exception as e:
        logger.error(f"\n‚úó ERREUR PIPELINE: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Pipeline ETL pour donn√©es accidents routiers"
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Force le t√©l√©chargement m√™me si fichiers existent"
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="Ignore l'√©tape de t√©l√©chargement"
    )
    
    args = parser.parse_args()
    
    success = run_pipeline(
        force_download=args.force_download,
        skip_download=args.skip_download
    )
    
    exit(0 if success else 1)
