"""
Script de t√©l√©chargement automatique des donn√©es d'accidents
avec v√©rification de hash et gestion des mises √† jour
"""

import os
import hashlib
import requests
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, Optional
import logging

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Chemins
BASE_DIR = Path(__file__).parent.parent.parent
RAW_DATA_DIR = BASE_DIR / "data" / "raw"
METADATA_FILE = RAW_DATA_DIR / ".metadata.json"

# URLs des datasets (√† adapter selon les sources r√©elles)
DATASETS = {
    "accidents": {
        "url": "https://www.data.gouv.fr/api/1/datasets/5f2e7ffa94a0c2558f5c25ea",
        "filename": "accidents.csv"
    },
    "caracteristiques": {
        "url": "https://www.data.gouv.fr/api/1/datasets/5f2e7ffa94a0c2558f5c25ea",
        "filename": "caracteristiques.csv"
    },
    "lieux": {
        "url": "https://www.data.gouv.fr/api/1/datasets/5f2e7ffa94a0c2558f5c25ea",
        "filename": "lieux.csv"
    },
    "usagers": {
        "url": "https://www.data.gouv.fr/api/1/datasets/5f2e7ffa94a0c2558f5c25ea",
        "filename": "usagers.csv"
    },
    "vehicules": {
        "url": "https://www.data.gouv.fr/api/1/datasets/5f2e7ffa94a0c2558f5c25ea",
        "filename": "vehicules.csv"
    }
}


def ensure_raw_data_dir():
    """Cr√©e le r√©pertoire des donn√©es brutes s'il n'existe pas"""
    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"‚úì R√©pertoire donn√©es: {RAW_DATA_DIR}")


def calculate_file_hash(file_path: Path, algorithm: str = "md5") -> str:
    """
    Calcule le hash d'un fichier
    
    Args:
        file_path: Chemin du fichier
        algorithm: Algorithme de hash (md5, sha256)
    
    Returns:
        Hash en hexad√©cimal
    """
    hash_obj = hashlib.new(algorithm)
    
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_obj.update(chunk)
    
    return hash_obj.hexdigest()


def calculate_url_hash(url: str) -> str:
    """
    Calcule le hash du contenu d'une URL
    
    Args:
        url: URL du fichier
    
    Returns:
        Hash en hexad√©cimal
    """
    try:
        response = requests.get(url, timeout=30, stream=True)
        response.raise_for_status()
        
        hash_obj = hashlib.md5()
        for chunk in response.iter_content(chunk_size=4096):
            if chunk:
                hash_obj.update(chunk)
        
        return hash_obj.hexdigest()
    except Exception as e:
        logger.error(f"‚úó Erreur calcul hash pour {url}: {e}")
        return None


def load_metadata() -> Dict:
    """Charge les m√©tadonn√©es de t√©l√©chargement"""
    if METADATA_FILE.exists():
        try:
            with open(METADATA_FILE, "r") as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"‚ö† Erreur lecture metadata: {e}")
    return {}


def save_metadata(metadata: Dict):
    """Sauvegarde les m√©tadonn√©es de t√©l√©chargement"""
    try:
        with open(METADATA_FILE, "w") as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"‚úì M√©tadonn√©es sauvegard√©es")
    except Exception as e:
        logger.error(f"‚úó Erreur sauvegarde metadata: {e}")


def download_file(url: str, file_path: Path, timeout: int = 30) -> Tuple[bool, str]:
    """
    T√©l√©charge un fichier avec barre de progression
    
    Args:
        url: URL du fichier
        file_path: Chemin de destination
        timeout: Timeout en secondes
    
    Returns:
        Tuple (succ√®s, message)
    """
    try:
        logger.info(f"üì• T√©l√©chargement: {file_path.name}")
        
        response = requests.get(url, timeout=timeout, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(file_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        print(f"\r  Progression: {progress:.1f}%", end="")
        
        print()  # Nouvelle ligne apr√®s la barre de progression
        logger.info(f"‚úì T√©l√©chargement r√©ussi: {file_path.name}")
        
        return True, "Succ√®s"
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚úó Erreur t√©l√©chargement: {e}")
        return False, str(e)
    except Exception as e:
        logger.error(f"‚úó Erreur inattendue: {e}")
        return False, str(e)


def should_update(file_path: Path, remote_hash: str, metadata: Dict) -> bool:
    """
    V√©rifie si le fichier doit √™tre mis √† jour
    
    Args:
        file_path: Chemin du fichier local
        remote_hash: Hash du fichier distant
        metadata: M√©tadonn√©es pr√©c√©dentes
    
    Returns:
        True si mise √† jour n√©cessaire
    """
    filename = file_path.name
    
    # Si fichier n'existe pas
    if not file_path.exists():
        logger.info(f"  ‚Üí Fichier n'existe pas, t√©l√©chargement n√©cessaire")
        return True
    
    # Si hash distant non disponible
    if not remote_hash:
        logger.warning(f"  ‚Üí Hash distant indisponible, forcer t√©l√©chargement")
        return True
    
    # Si hash local existe dans metadata
    local_hash = metadata.get("files", {}).get(filename, {}).get("hash")
    
    if local_hash == remote_hash:
        logger.info(f"  ‚Üí Hash identique, pas de mise √† jour")
        return False
    else:
        logger.info(f"  ‚Üí Hash diff√©rent, mise √† jour n√©cessaire")
        return True


def process_dataset_resources(dataset_name: str, dataset_url: str) -> Dict[str, str]:
    """
    R√©cup√®re les ressources (fichiers CSV) d'un dataset
    
    Args:
        dataset_name: Nom du dataset
        dataset_url: URL de l'API dataset
    
    Returns:
        Dict {filename: download_url}
    """
    resources = {}
    
    try:
        response = requests.get(dataset_url, timeout=10)
        response.raise_for_status()
        
        dataset = response.json()
        
        for resource in dataset.get("resources", []):
            # Filtrer les fichiers CSV
            if resource.get("format", "").upper() == "CSV":
                filename = resource.get("title", resource.get("name", "unknown.csv"))
                if not filename.endswith(".csv"):
                    filename += ".csv"
                
                url = resource.get("url")
                if url:
                    resources[filename] = url
                    logger.info(f"  Ressource trouv√©e: {filename}")
        
        logger.info(f"‚úì {len(resources)} ressources trouv√©es pour {dataset_name}")
        
    except Exception as e:
        logger.error(f"‚úó Erreur lors de la r√©cup√©ration des ressources: {e}")
    
    return resources


def download_all_datasets(force: bool = False) -> Dict[str, Dict]:
    """
    T√©l√©charge tous les datasets d'accidents
    
    Args:
        force: Force le t√©l√©chargement m√™me si fichier existe
    
    Returns:
        Dict des r√©sultats de t√©l√©chargement
    """
    ensure_raw_data_dir()
    
    metadata = load_metadata()
    if "files" not in metadata:
        metadata["files"] = {}
    
    results = {}
    
    logger.info("\n" + "=" * 80)
    logger.info("üì• T√âL√âCHARGEMENT DONN√âES ACCIDENTS ROUTIERS")
    logger.info("=" * 80 + "\n")
    
    # Traiter chaque dataset
    for dataset_name, dataset_config in DATASETS.items():
        logger.info(f"\nüîç Dataset: {dataset_name}")
        logger.info("-" * 40)
        
        dataset_url = dataset_config["url"]
        filename = dataset_config["filename"]
        file_path = RAW_DATA_DIR / filename
        
        # R√©cup√©rer les ressources
        resources = process_dataset_resources(dataset_name, dataset_url)
        
        if not resources:
            logger.warning(f"‚ö† Aucune ressource trouv√©e pour {dataset_name}")
            results[dataset_name] = {
                "success": False,
                "message": "Aucune ressource trouv√©e"
            }
            continue
        
        # Prendre la premi√®re ressource CSV (adapter si besoin)
        download_url = list(resources.values())[0]
        
        # V√©rifier hash distant
        logger.info(f"üîê Calcul du hash distant...")
        remote_hash = calculate_url_hash(download_url)
        
        # V√©rifier si mise √† jour n√©cessaire
        if force or should_update(file_path, remote_hash, metadata):
            success, message = download_file(download_url, file_path)
            
            if success:
                # Calculer hash local
                local_hash = calculate_file_hash(file_path)
                
                # Mettre √† jour m√©tadonn√©es
                metadata["files"][filename] = {
                    "hash": local_hash,
                    "remote_hash": remote_hash,
                    "download_url": download_url,
                    "downloaded_at": datetime.now().isoformat(),
                    "size": file_path.stat().st_size
                }
                
                results[dataset_name] = {
                    "success": True,
                    "message": f"T√©l√©charg√© avec succ√®s",
                    "size": file_path.stat().st_size,
                    "hash": local_hash
                }
            else:
                results[dataset_name] = {
                    "success": False,
                    "message": message
                }
        else:
            logger.info(f"‚úì Fichier √† jour, pas de t√©l√©chargement")
            results[dataset_name] = {
                "success": True,
                "message": "Fichier d√©j√† √† jour"
            }
    
    # Sauvegarder m√©tadonn√©es
    save_metadata(metadata)
    
    # Afficher r√©sum√©
    logger.info("\n" + "=" * 80)
    logger.info("üìä R√âSUM√â T√âL√âCHARGEMENT")
    logger.info("=" * 80)
    
    for dataset, result in results.items():
        status = "‚úì" if result.get("success") else "‚úó"
        logger.info(f"{status} {dataset}: {result.get('message')}")
    
    return results


if __name__ == "__main__":
    import sys
    
    # Permettre le for√ßage du t√©l√©chargement avec --force
    force_download = "--force" in sys.argv
    
    results = download_all_datasets(force=force_download)
    
    # Code de sortie
    all_success = all(r.get("success") for r in results.values())
    exit(0 if all_success else 1)
