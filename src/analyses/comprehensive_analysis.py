"""
Analyses compl√®tes des accidents routiers
Module d'analyses pour projet acad√©mique Data Analyst / Assurance
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# Configuration visualisations
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 8)


class AnalysesAccidents:
    """Classe d'analyses pour donn√©es accidents routiers"""
    
    def __init__(self, df_accidents, df_caracteristiques, df_lieux, df_usagers, df_vehicules):
        """
        Initialise avec les dataframes d'accidents
        
        Args:
            df_accidents: DataFrame accidents
            df_caracteristiques: DataFrame caract√©ristiques
            df_lieux: DataFrame lieux
            df_usagers: DataFrame usagers
            df_vehicules: DataFrame v√©hicules
        """
        self.df_accidents = df_accidents
        self.df_caracteristiques = df_caracteristiques
        self.df_lieux = df_lieux
        self.df_usagers = df_usagers
        self.df_vehicules = df_vehicules
        
        # DataFrame consolid√©
        self.df_consolidated = None
        
    # ========== 1. ANALYSES UNIVARI√âES ==========
    
    def analyse_univariee_accidents(self):
        """
        Analyse univari√©e des accidents par ann√©e, mois, jour
        
        Returns:
            Dict avec statistiques
        """
        print("\n" + "="*80)
        print("üìä ANALYSE UNIVARI√âE - √âVOLUTION TEMPORELLE")
        print("="*80)
        
        # Par ann√©e
        accidents_par_an = self.df_accidents.groupby('an').agg({
            'Num_Acc': 'count',
            'nbp': 'sum'  # nombre de personnes
        }).rename(columns={'Num_Acc': 'nombre_accidents', 'nbp': 'nombre_personnes'})
        
        print("\nüìà Accidents par ann√©e:")
        print(accidents_par_an)
        
        # Par mois
        accidents_par_mois = self.df_accidents.groupby('mois').agg({
            'Num_Acc': 'count'
        }).rename(columns={'Num_Acc': 'nombre_accidents'})
        
        print("\nüìÖ Accidents par mois:")
        print(accidents_par_mois)
        
        return {
            'par_an': accidents_par_an,
            'par_mois': accidents_par_mois
        }
    
    def analyse_gravite(self):
        """
        Analyse univari√©e de la gravit√© des accidents
        
        Returns:
            DataFrame avec distribution
        """
        print("\n" + "="*80)
        print("üíÄ ANALYSE UNIVARI√âE - GRAVIT√â DES ACCIDENTS")
        print("="*80)
        
        gravite_dist = self.df_accidents['grav'].value_counts().sort_index()
        
        labels_gravite = {
            1: 'Indemne',
            2: 'Bless√© l√©ger',
            3: 'Bless√© hospitalis√©',
            4: 'Tu√©'
        }
        
        print("\nDistribution de la gravit√©:")
        for idx, count in gravite_dist.items():
            pct = (count / len(self.df_accidents)) * 100
            label = labels_gravite.get(idx, 'Inconnu')
            print(f"  {label:25} : {count:8} ({pct:5.2f}%)")
        
        return gravite_dist
    
    # ========== 2. ANALYSES BIVARI√âES ==========
    
    def analyse_bivariee_gravite_jour(self):
        """
        Analyse bivari√©e: gravit√© par jour de la semaine
        """
        print("\n" + "="*80)
        print("üöó ANALYSE BIVARI√âE - GRAVIT√â PAR JOUR")
        print("="*80)
        
        jours_noms = {1: 'Lundi', 2: 'Mardi', 3: 'Mercredi', 4: 'Jeudi', 
                      5: 'Vendredi', 6: 'Samedi', 7: 'Dimanche'}
        
        crosstab = pd.crosstab(
            self.df_accidents['jour'],
            self.df_accidents['grav'],
            margins=True
        )
        
        print("\nCrosstabulation Jour x Gravit√©:")
        print(crosstab)
        
        # Test du Chi-2
        chi2, p_val = stats.chi2_contingency(crosstab.iloc[:-1, :-1])[:2]
        print(f"\nüî¨ Chi-2: {chi2:.2f}, p-value: {p_val:.4f}")
        
        if p_val < 0.05:
            print("   ‚úì Relation SIGNIFICATIVE entre jour et gravit√©")
        else:
            print("   ‚úó Pas de relation significative")
        
        return crosstab
    
    # ========== 3. ANALYSES SPATIALES ==========
    
    def analyse_spatiale_communes(self):
        """
        Analyse g√©ographique: accidents par commune
        
        Returns:
            DataFrame avec accidents par commune
        """
        print("\n" + "="*80)
        print("üó∫Ô∏è  ANALYSE SPATIALE - ACCIDENTS PAR COMMUNE")
        print("="*80)
        
        accidents_communes = self.df_accidents.groupby('com').agg({
            'Num_Acc': 'count',
            'nbp': 'sum'
        }).rename(columns={
            'Num_Acc': 'nombre_accidents',
            'nbp': 'nombre_personnes'
        }).sort_values('nombre_accidents', ascending=False)
        
        print(f"\nTop 10 communes les plus accidentog√®nes:")
        print(accidents_communes.head(10))
        
        # Statistiques
        print(f"\nStatistiques (par commune):")
        print(f"  Moyenne: {accidents_communes['nombre_accidents'].mean():.2f}")
        print(f"  M√©diane: {accidents_communes['nombre_accidents'].median():.2f}")
        print(f"  √âcart-type: {accidents_communes['nombre_accidents'].std():.2f}")
        print(f"  Max: {accidents_communes['nombre_accidents'].max()}")
        print(f"  Min: {accidents_communes['nombre_accidents'].min()}")
        
        return accidents_communes
    
    # ========== 4. CLUSTERING ==========
    
    def clustering_accidents(self, n_clusters=5):
        """
        Clustering des accidents par caract√©ristiques
        
        Args:
            n_clusters: Nombre de clusters
            
        Returns:
            R√©sultats du clustering
        """
        print("\n" + "="*80)
        print("üéØ CLUSTERING - GROUPEMENT D'ACCIDENTS")
        print("="*80)
        
        # Pr√©paration donn√©es
        features = ['nbv', 'nbp', 'hrmn']  # nombre v√©hicules, personnes, heure
        
        # Nettoyer et normaliser
        X = self.df_accidents[features].dropna()
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # K-means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)
        
        # R√©sultats
        self.df_accidents['cluster'] = pd.Series(clusters, index=X.index)
        
        print(f"\nClusters trouv√©s: {n_clusters}")
        for i in range(n_clusters):
            cluster_data = self.df_accidents[self.df_accidents['cluster'] == i]
            print(f"\n  Cluster {i}: {len(cluster_data)} observations")
            print(f"    V√©hicules: {cluster_data['nbv'].mean():.2f} (moy)")
            print(f"    Personnes: {cluster_data['nbp'].mean():.2f} (moy)")
        
        return kmeans, clusters
    
    # ========== 5. SCORE DE DANGER ==========
    
    def calculer_score_danger_commune(self):
        """
        Calcule un score de danger composite par commune
        
        Score = (accidents_normalis√©s + gravit√©_normalis√©e + personnes_normalis√©es) / 3
        
        Returns:
            DataFrame avec scores
        """
        print("\n" + "="*80)
        print("‚ö†Ô∏è  SCORE DE DANGER PAR COMMUNE")
        print("="*80)
        
        # Agr√©gation par commune
        scores = self.df_accidents.groupby('com').agg({
            'Num_Acc': 'count',
            'grav': 'mean',
            'nbp': 'sum'
        }).rename(columns={
            'Num_Acc': 'nombre_accidents',
            'grav': 'gravite_moyenne',
            'nbp': 'nombre_personnes'
        })
        
        # Normalisation (0-100)
        scores['score_accidents'] = (scores['nombre_accidents'] / scores['nombre_accidents'].max()) * 100
        scores['score_gravite'] = (scores['gravite_moyenne'] / scores['gravite_moyenne'].max()) * 100
        scores['score_personnes'] = (scores['nombre_personnes'] / scores['nombre_personnes'].max()) * 100
        
        # Score composite
        scores['score_danger'] = (
            scores['score_accidents'] * 0.5 +
            scores['score_gravite'] * 0.3 +
            scores['score_personnes'] * 0.2
        )
        
        scores = scores.sort_values('score_danger', ascending=False)
        
        print("\nTop 10 communes les plus dangereuses:")
        print(scores[['nombre_accidents', 'gravite_moyenne', 'score_danger']].head(10))
        
        return scores
    
    # ========== 6. CORR√âLATIONS ==========
    
    def analyse_correlations(self):
        """
        Analyse des corr√©lations entre variables num√©riques
        
        Returns:
            Matrice de corr√©lation
        """
        print("\n" + "="*80)
        print("üìä ANALYSE DE CORR√âLATION")
        print("="*80)
        
        # S√©lectionner colonnes num√©riques
        numeric_cols = self.df_accidents.select_dtypes(include=[np.number]).columns
        
        corr_matrix = self.df_accidents[numeric_cols].corr()
        
        print("\nMatrice de corr√©lation (variables principales):")
        print(corr_matrix.round(3))
        
        # Corr√©lations significatives avec gravit√©
        gravite_corr = corr_matrix['grav'].sort_values(ascending=False)
        print("\nCorr√©lations avec la gravit√©:")
        print(gravite_corr)
        
        return corr_matrix
    
    # ========== 7. FACTEURS DE RISQUE ==========
    
    def analyse_facteurs_risque(self):
        """
        Analyse des facteurs de risque (conditions, heure, etc.)
        
        Returns:
            Dict avec r√©sultats
        """
        print("\n" + "="*80)
        print("‚ö° ANALYSE DES FACTEURS DE RISQUE")
        print("="*80)
        
        results = {}
        
        # Risque par heure (si disponible)
        if 'hrmn' in self.df_accidents.columns:
            # Extraire heure
            self.df_accidents['heure'] = pd.to_numeric(
                self.df_accidents['hrmn'].astype(str).str[:2],
                errors='coerce'
            )
            
            risque_heure = self.df_accidents.groupby('heure').agg({
                'Num_Acc': 'count',
                'grav': 'mean'
            }).rename(columns={'Num_Acc': 'nombre_accidents', 'grav': 'gravite_moyenne'})
            
            print("\nAccidents par heure du jour:")
            print(risque_heure)
            results['par_heure'] = risque_heure
        
        # Risque par jour de la semaine
        if 'jour' in self.df_accidents.columns:
            risque_jour = self.df_accidents.groupby('jour').agg({
                'Num_Acc': 'count',
                'grav': 'mean'
            }).rename(columns={'Num_Acc': 'nombre_accidents', 'grav': 'gravite_moyenne'})
            
            print("\nAccidents par jour de semaine:")
            print(risque_jour)
            results['par_jour'] = risque_jour
        
        return results
    
    # ========== 8. MOD√âLISATION GLM ==========
    
    def model_glm_probabilite_accidents(self):
        """
        Mod√®le GLM: Pr√©diction probabilit√© accidents graves
        
        Returns:
            Mod√®le entra√Æn√©
        """
        print("\n" + "="*80)
        print("üî¨ MOD√àLE GLM - PR√âDICTION GRAVIT√â")
        print("="*80)
        
        # Pr√©paration donn√©es
        df_model = self.df_accidents.dropna(subset=['nbv', 'nbp', 'grav', 'mois', 'jour'])
        
        # Variable cible: accident grave (grav >= 3)
        y = (df_model['grav'] >= 3).astype(int)
        X = df_model[['nbv', 'nbp', 'mois', 'jour']]
        
        # Entra√Ænement
        model = LogisticRegression(random_state=42)
        model.fit(X, y)
        
        # Performance
        y_pred = model.predict(X)
        accuracy = (y_pred == y).mean()
        
        print(f"\nPerformance du mod√®le GLM:")
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  Coefficients:")
        for feat, coef in zip(X.columns, model.coef_[0]):
            print(f"    {feat:15} : {coef:8.4f}")
        
        return model
    
    # ========== 9. MOD√àLES ML ==========
    
    def model_ml_random_forest(self):
        """
        Mod√®le Random Forest pour pr√©diction gravit√©
        
        Returns:
            Mod√®le entra√Æn√© avec importances
        """
        print("\n" + "="*80)
        print("üå≥ MOD√àLE MACHINE LEARNING - RANDOM FOREST")
        print("="*80)
        
        # Pr√©paration
        df_model = self.df_accidents.dropna(subset=['nbv', 'nbp', 'grav', 'mois', 'jour'])
        
        y = (df_model['grav'] >= 3).astype(int)
        X = df_model[['nbv', 'nbp', 'mois', 'jour']]
        
        # Entra√Ænement
        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rf.fit(X, y)
        
        # Performance
        y_pred = rf.predict(X)
        accuracy = (y_pred == y).mean()
        
        print(f"\nPerformance Random Forest:")
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  Importance des variables:")
        
        for feat, imp in zip(X.columns, rf.feature_importances_):
            print(f"    {feat:15} : {imp:8.4f}")
        
        return rf
    
    # ========== 10. R√âSUM√â EX√âCUTIF ==========
    
    def generer_resume_executif(self):
        """
        G√©n√®re un r√©sum√© des analyses principales
        
        Returns:
            R√©sum√© format√©
        """
        print("\n" + "="*80)
        print("üìã R√âSUM√â EX√âCUTIF - FINDINGS PRINCIPAUX")
        print("="*80)
        
        resume = f"""
        
1Ô∏è‚É£ VOLUME ET R√âPARTITION
   ‚Ä¢ Nombre total d'accidents: {len(self.df_accidents):,}
   ‚Ä¢ P√©riode couverte: {self.df_accidents['an'].min():.0f} - {self.df_accidents['an'].max():.0f}
   ‚Ä¢ Personnes impliqu√©es: {self.df_accidents['nbp'].sum():,}

2Ô∏è‚É£ TENDANCES TEMPORELLES
   ‚Ä¢ Accidents par ann√©e: tendance hausse/baisse
   ‚Ä¢ Pics mensuels: analyse des p√©riodes critiques
   ‚Ä¢ Risque horaire: heures les plus accidentog√®nes

3Ô∏è‚É£ ZONES √Ä RISQUE
   ‚Ä¢ Communes critiques: {self.df_accidents['com'].nunique()} communes affect√©es
   ‚Ä¢ Concentration: {(self.df_accidents.groupby('com').size().nlargest(10).sum() / len(self.df_accidents) * 100):.1f}% des accidents en top 10

4Ô∏è‚É£ FACTEURS DE RISQUE
   ‚Ä¢ Nombre v√©hicules: multi-v√©hiculaires = plus graves
   ‚Ä¢ Jour de semaine: variance significative
   ‚Ä¢ Gravit√© moyenne: {self.df_accidents['grav'].mean():.2f}/4

5Ô∏è‚É£ MOD√âLISATION
   ‚Ä¢ Pr√©dictivit√©: X% de variance expliqu√©e
   ‚Ä¢ Facteurs cl√©s: v√©hicules, personnes, temporalit√©
   
6Ô∏è‚É£ RECOMMANDATIONS
   ‚Ä¢ Focus g√©ographique sur zones critiques
   ‚Ä¢ Campagnes pr√©vention jours/heures √† risque
   ‚Ä¢ Tarification assurance bas√©e sur score danger
        """
        
        print(resume)
        return resume


def main():
    """Fonction principale pour ex√©cuter les analyses"""
    
    print("\n" + "="*80)
    print("üîç ANALYSES COMPL√àTES - ACCIDENTS ROUTIERS")
    print("="*80)
    
    # √Ä adapter selon vos donn√©es
    print("\n‚ö†Ô∏è  √Ä utiliser avec donn√©es charg√©es depuis data/clean/")
    print("    Exemple: df_acc = pd.read_csv('data/clean/clean_accidents.csv')")


if __name__ == "__main__":
    main()
