# üöÄ Guide Installation Airflow

## Installation Airflow dans le venv

### 1Ô∏è‚É£ Installation des d√©pendances

```bash
# Activer le venv
source venv/bin/activate

# Installer Airflow (peut prendre quelques minutes)
pip install -r requirements.txt

# Ou installation minimale
pip install apache-airflow==2.7.3 apache-airflow-providers-postgres==5.10.0
```

### 2Ô∏è‚É£ Initialiser Airflow

```bash
# D√©finir le r√©pertoire Airflow
export AIRFLOW_HOME=/home/sdd/projetetudeapi/airflow_home

# Initialiser la base de donn√©es
airflow db init

# Cr√©er un utilisateur admin
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@accidents.local \
  --password password123
```

### 3Ô∏è‚É£ D√©marrer Airflow

```bash
# Terminal 1: WebServer (interface web)
airflow webserver --port 8080

# Terminal 2 (nouveau): Scheduler (planificateur)
airflow scheduler
```

### 4Ô∏è‚É£ Acc√©der √† l'interface

Ouvre dans ton navigateur: **http://localhost:8080**

- **Identifiant**: admin
- **Mot de passe**: password123

---

## üìã Utilisation des DAGs

### Lister les DAGs

```bash
airflow dags list
```

**R√©sultat attendu:**
```
dag_id                  | filepath
accidents_etl_pipeline  | /dags/accidents_pipeline.py
accidents_maintenance   | /dags/maintenance.py
```

### Tester une DAG

```bash
# Test sans ex√©cuter les d√©pendances
airflow dags test accidents_etl_pipeline 2024-01-01

# Test d'une t√¢che sp√©cifique
airflow tasks test accidents_etl_pipeline download_data 2024-01-01
```

### Ex√©cuter une DAG manuellement

```bash
# D√©clencher une ex√©cution manuelle
airflow dags trigger -e 2024-01-01 accidents_etl_pipeline

# V√©rifier le statut
airflow dags list-runs --dag-id accidents_etl_pipeline
```

### Monitorer l'ex√©cution

Via l'interface web:
1. Aller √†: **Dags ‚Üí accidents_etl_pipeline**
2. Cliquer sur **Graph View** pour voir les t√¢ches
3. Cliquer sur une t√¢che pour voir les logs

---

## üîÑ DAGs Disponibles

### 1. `accidents_etl_pipeline`

**Orchestration du pipeline ETL complet**

```
D√©but
  ‚Üì
V√©rifier r√©pertoires
  ‚Üì
T√©l√©charger donn√©es (data.gouv.fr)
  ‚Üì
Nettoyer donn√©es
  ‚Üì
Charger PostgreSQL
  ‚Üì
Valider donn√©es
  ‚Üì
Nettoyer fichiers temp
  ‚Üì
Fin
```

**Planification**: Lundi 3h du matin

**Commandes utiles**:
```bash
# Voir la structure de la DAG
airflow dags show accidents_etl_pipeline

# Lancer imm√©diatement
airflow dags trigger -e 2024-01-01 accidents_etl_pipeline

# Voir les ex√©cutions pass√©es
airflow dags list-runs --dag-id accidents_etl_pipeline
```

### 2. `accidents_maintenance`

**Maintenance et monitoring du syst√®me**

```
D√©but
  ‚Üì
V√©rification sant√© BD
V√©rification espace disque
  ‚Üì
Backup base de donn√©es
  ‚Üì
Nettoyage anciens backups
  ‚Üì
G√©n√©rer rapport
  ‚Üì
Fin
```

**Planification**: Chaque jour √† 1h du matin

**Fichiers cr√©√©s**:
- Backups: `/home/sdd/projetetudeapi/backups/accidents_db_*.sql`
- Rapport: `/tmp/maintenance_report.txt`

---

## üêõ Troubleshooting

### Erreur: "Airflow Home is not defined"

```bash
export AIRFLOW_HOME=/home/sdd/projetetudeapi/airflow_home
```

### Erreur: "No module named 'src'"

```bash
# V√©rifier que tu es dans le bon r√©pertoire
cd /home/sdd/projetetudeapi

# Ajouter au PYTHONPATH
export PYTHONPATH=/home/sdd/projetetudeapi:$PYTHONPATH
```

### Erreur de connexion PostgreSQL

```bash
# V√©rifier la config dans src/config.py
# S'assurer que PostgreSQL tourne
pg_isready -h localhost -p 5432

# Tester la connexion
psql -h localhost -U postgres -d accidents
```

### R√©initialiser Airflow

```bash
# ‚ö†Ô∏è Attention: Supprime tout l'historique!
rm -rf airflow_home/
export AIRFLOW_HOME=/home/sdd/projetetudeapi/airflow_home
airflow db init
```

---

## üìä Monitoring Avanc√©

### Logs d√©taill√©s

```bash
# Logs d'une DAG sp√©cifique
tail -f airflow_home/logs/accidents_etl_pipeline/

# Logs d'une t√¢che
tail -f airflow_home/logs/accidents_etl_pipeline/download_data/
```

### M√©triques Airflow

Via le webUI: **Admin ‚Üí Logs** ou **Admin ‚Üí Metrics**

### Configuration PostgreSQL pour Airflow

Airflow utilise une BD pour stocker l'√©tat des DAGs (d√©faut: SQLite)

Pour utiliser PostgreSQL comme m√©tabase Airflow:

```bash
# Installer le provider
pip install apache-airflow-providers-postgres

# Configurer dans airflow.cfg
sql_alchemy_conn = postgresql+psycopg2://user:password@localhost:5432/airflow
```

---

## üöÄ D√©ploiement Production

### Sur Render

```bash
# Procfile (ajouter les deux processus)
webserver: airflow webserver --port $PORT
scheduler: airflow scheduler
```

### Variables d'environnement Airflow

```bash
# .env ou variables Render
AIRFLOW_HOME=/app/airflow_home
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://...
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__DAGS_FOLDER=/app/dags
AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
```

---

## ‚úÖ Checklist

- [ ] Airflow install√©: `pip install apache-airflow`
- [ ] Environnement: `export AIRFLOW_HOME=...`
- [ ] BD initialis√©e: `airflow db init`
- [ ] Admin cr√©√©: `airflow users create --username admin ...`
- [ ] Webserver d√©marr√©: `airflow webserver --port 8080`
- [ ] Scheduler d√©marr√©: `airflow scheduler`
- [ ] DAGs visibles: `http://localhost:8080`
- [ ] DAG accidents_etl_pipeline test√©e
- [ ] DAG accidents_maintenance test√©e

---

## üìö Ressources

- [Airflow Docs](https://airflow.apache.org/)
- [Airflow Providers](https://registry.astronomer.io/)
- [Airflow CLI](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html)

---

**Pr√™t √† orchestrer ton pipeline! üöÄ**
