# ğŸ“Š Guide Analyses AcadÃ©miques - Accidents Routiers

## Contexte Projet

**RÃ´le**: Data Analyst au sein d'une sociÃ©tÃ© d'assurance  
**Objectif**: Analyser les bases de donnÃ©es Open Data des accidents corporels pour gÃ©rer le risque de responsabilitÃ© civile  
**Ã‰valuation**: Projets "Traitement de DonnÃ©es", "Machine Learning", "EconomÃ©trie"

---

## ğŸ“‹ Consignes AcadÃ©miques

### Fond (Contenu)

âœ… **Utiliser packages Python** (vus ou non) pour exploiter les donnÃ©es  
âœ… **Analyses pertinentes** avec sens et intÃ©rÃªt (pas juste du code)  
âœ… **InterprÃ©ter rÃ©sultats** : dÃ©duire relations, tendances, patterns  
âœ… **Expliciter chaque analyse** : objectif â†’ mise en place â†’ rÃ©sultats  
âœ… **Variables externes** :
   - RÃ©gion (data.gouv.fr)
   - DensitÃ©/population (INSEE)
   - Analyse gÃ©ographique pour zonier

### Forme (PrÃ©sentation)

âœ… **Notebook propre** : clair, prÃ©cis, structurÃ©, concis  
âœ… **Commentaires et titres** de parties  
âœ… **Conclusions et interprÃ©tations**  
âœ… **Note PDF** en plus du notebook  
âœ… **Code commentÃ©** avec documentation

---

## ğŸ“Š Processus Classique d'Analyse Statistique

```
1. Nettoyage de donnÃ©es
   â”œâ”€ ComprÃ©hension format
   â”œâ”€ Analyse complÃ©tude
   â”œâ”€ ContrÃ´les cohÃ©rence
   â””â”€ Gestion anomalies

2. Jointure et enrichissement
   â”œâ”€ Variables externes (rÃ©gion, densitÃ©)
   â”œâ”€ Rapprochement donnÃ©es
   â””â”€ Gestion doublons

3. Analyse descriptive
   â”œâ”€ UnivariÃ©e (distributions)
   â”œâ”€ BivariÃ©e (crosstab, corrÃ©lations)
   â””â”€ MultivariÃ©e (clustering, ACP)

4. ModÃ©lisation
   â”œâ”€ Approche paramÃ©trique (GLM)
   â””â”€ Approche non-paramÃ©trique (ML)

5. RÃ©sultats et interprÃ©tation
   â”œâ”€ Analyse coefficients
   â”œâ”€ Tests statistiques
   â””â”€ Implications pratiques

6. Applications
   â”œâ”€ PrÃ©visions
   â”œâ”€ Tarification
   â””â”€ Gestion des risques
```

---

## ğŸ¯ 12 Sections du Notebook

### 1ï¸âƒ£ **Chargement et Exploration**
- Charger datasets (5 fichiers accidents)
- Structure et dimensions
- Statistiques basiques

**Objectif**: Comprendre les donnÃ©es  
**Code**: `pd.read_csv()`, `.info()`, `.describe()`, `.head()`

---

### 2ï¸âƒ£ **Nettoyage et PrÃ©traitement**
- Analyse complÃ©tude (% valeurs manquantes)
- Doublons (identification et suppression)
- IncohÃ©rences (inter-variables)
- Corrections univariÃ©es/multivariÃ©es

**MÃ©trique**: 
```
Avant nettoyage: X lignes, Y % manquant
AprÃ¨s nettoyage: X' lignes, Y'% manquant
```

---

### 3ï¸âƒ£ **IntÃ©gration Variables Externes**
Ajouter:
- **RÃ©gions**: joindre communes â†’ rÃ©gions
- **DÃ©mographie**: population, densitÃ© (INSEE)
- **GÃ©ographie**: latitude/longitude (lieux)

**Validation**: Afficher % rapprochement rÃ©ussi

---

### 4ï¸âƒ£ **Analyse UnivariÃ©e**
Pour chaque variable clÃ©:
- Histogramme/distribution
- Statistiques (mean, median, std, quantiles)
- Cas particuliers (0, nÃ©gatifs, outliers)

**Variables**: accidents/an, gravitÃ©, vÃ©hicules, personnes, heure, jour

---

### 5ï¸âƒ£ **Analyse BivariÃ©e et MultivariÃ©e**
- Crosstabs (jour Ã— gravitÃ©, etc.)
- **Tests Chi-2** pour variables catÃ©goriques
- **Correlations** (Pearson/Spearman) numÃ©riques
- **Visualisations** (heatmaps, scatter, boxplots)

**Exemple**:
```python
# GravitÃ© par jour (test significativitÃ©)
crosstab = pd.crosstab(df['jour'], df['grav'])
chi2, p_val = stats.chi2_contingency(crosstab)
```

---

### 6ï¸âƒ£ **Analyse Spatiale et Heatmaps**
- Top 20 communes dangereuses (par accidents)
- **Heatmap gÃ©ographique**: latitude Ã— longitude Ã— accidents
- DensitÃ© zones urbaines vs rurales
- Clustering spatial (si coordonnÃ©es disponibles)

**Visualisation**:
```python
plt.hexbin(df['lat'], df['lon'], gridsize=30, cmap='YlOrRd')
```

---

### 7ï¸âƒ£ **Clustering (Classification Non-SupervisÃ©e)**
- **K-Means**: grouper accidents par caractÃ©ristiques
  - Features: nombre vÃ©hicules, personnes, heure, etc.
  - DÃ©terminer k optimal (elbow method)
- **Hierarchical clustering**: dendrogramme

**InterprÃ©tation**: Profil de chaque cluster

---

### 8ï¸âƒ£ **Score de Danger par Commune**
CrÃ©er score composite (0-100):
```
Score = 50% Ã— (accidents normalisÃ©s) 
       + 30% Ã— (gravitÃ© moyenne)
       + 20% Ã— (nombre personnes)
```

**Output**: Ranking communes (plus dangereuses d'abord)

---

### 9ï¸âƒ£ **Analyse des Facteurs de Risque**
CorrÃ©lations avec accidents/gravitÃ©:
- â° **Heure du jour** (pics matinaux, soir?)
- ğŸ“… **Jour de semaine** (WE vs semaine?)
- ğŸŒ¤ï¸ **MÃ©tÃ©o** (si disponible)
- ğŸ›£ï¸ **Infrastructure** (type route)
- ğŸ™ï¸ **DensitÃ©** (urbain vs rural)

**Test**: ANOVA ou Kruskal-Wallis selon distribution

---

### ğŸ”Ÿ **ModÃ©lisation: Approche ParamÃ©trique (GLM)**
PrÃ©dire: Accident grave (gravitÃ© â‰¥ 3)?

```python
from sklearn.linear_model import LogisticRegression

y = (df['grav'] >= 3).astype(int)  # Variable cible
X = df[['nbv', 'nbp', 'mois', 'jour']]

model = LogisticRegression()
model.fit(X, y)
```

**RÃ©sultats**:
- Accuracy/AUC
- Coefficients (interprÃ©tation: impact variables)
- P-values (significativitÃ©)

---

### 1ï¸âƒ£1ï¸âƒ£ **ModÃ©lisation: Approche Non-ParamÃ©trique (ML)**
Comparer modÃ¨les:
- **Random Forest** (importance variables)
- **Gradient Boosting** (meilleure performance?)
- **Neural Networks** (si donnÃ©es + complexes)

**Comparaison**:
```
ModÃ¨le           | Accuracy | AUC   | F1-Score
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
Logistic         |   0.78   | 0.82  |  0.65
Random Forest    |   0.85   | 0.89  |  0.74
Gradient Boost   |   0.87   | 0.91  |  0.76
```

---

### 1ï¸âƒ£2ï¸âƒ£ **RÃ©sultats, InterprÃ©tation et Recommandations**

**SynthÃ¨se des findings:**
1. Tendances temporelles (accidents augmentent? oÃ¹?)
2. Zones critiques (communes/rÃ©gions Ã  surveiller)
3. Facteurs clÃ©s (heure, jour, infrastructure)
4. Patterns identifiÃ©s (qui cause les accidents graves?)

**Recommandations pour assurance:**
- ğŸ¯ Zones de surprime (score danger > X)
- â° Tarification horaire/jour
- ğŸš— Surprime multi-vÃ©hicules
- ğŸ“ Zonier gÃ©ographique
- ğŸ“Š Score creditant (ajustement primes)

---

## ğŸ“ˆ Exemple: Analyse ComplÃ¨te d'une Variable

### Cas: Accidents par Heure

**1. Objectif**
Identifier les heures critiques pour prÃ©vention et tarification

**2. MÃ©thodologie**
```python
# Extraire heure
df['heure'] = df['hrmn'].str[:2].astype(int)

# AgrÃ©gation
risque_heure = df.groupby('heure').agg({
    'Num_Acc': 'count',
    'grav': 'mean',
    'nbp': 'sum'
}).rename(columns={...})

# Visualisation
plt.figure(figsize=(12, 5))
plt.plot(risque_heure.index, risque_heure['Num_Acc'], marker='o')
plt.xlabel('Heure du jour')
plt.ylabel('Nombre d\'accidents')
plt.title('Profil horaire des accidents')
plt.grid(True)
```

**3. RÃ©sultats**
- Heures critiques: 8h (trajet travail), 17-19h (retour)
- GravitÃ©: pics diffÃ©rents (24h en campagne?)
- Pattern: week-end â‰  semaine

**4. InterprÃ©tation**
- Surcharge rÃ©seau certaines heures
- Fatigue/vigilance Ã  23h?
- Conditions mÃ©tÃ©o variables?

**5. Implication**
- Primes rÃ©duites heures creuses
- Campagnes prÃ©vention aux heures critiques
- Incitation trajets hors-pics

---

## ğŸ› ï¸ Stack Technique RecommandÃ©e

```python
# Data
import pandas as pd
import numpy as np

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Analyse
from scipy import stats
from scipy.spatial.distance import pdist, squareform

# ML
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Autres
import warnings
warnings.filterwarnings('ignore')
```

---

## ğŸ“ Checklist Notebook

- [ ] Introduction et contexte
- [ ] Chargement donnÃ©es
- [ ] Exploration (dimensions, types)
- [ ] Nettoyage (complÃ©tude, doublons, outliers)
- [ ] Variables externes intÃ©grÃ©es
- [ ] Analyses univariÃ©es (distributions, stats)
- [ ] Analyses bivariÃ©es (crosstabs, corrÃ©lations)
- [ ] Analyses multivariÃ©es (clustering, ACP?)
- [ ] Analyse spatiale (heatmaps, communes)
- [ ] Score de danger calculÃ© et rankingÃ©
- [ ] ModÃ¨le GLM (logistic regression)
- [ ] ModÃ¨les ML (Random Forest, XGBoost)
- [ ] Comparaison modÃ¨les
- [ ] RÃ©sultats interprÃ©tÃ©s
- [ ] Recommandations pour assurance
- [ ] Conclusions
- [ ] Code commentÃ© et documentÃ©

---

## ğŸ“„ Structure Note PDF

```
1. RÃ©sumÃ© exÃ©cutif (1 page)
   - Question posÃ©e
   - DonnÃ©es utilisÃ©es
   - Findings principaux
   - Recommandations

2. MÃ©thodologie (1-2 pages)
   - Sources donnÃ©es
   - Processus nettoyage
   - Variables externes
   - Approches analytiques

3. RÃ©sultats (5-10 pages)
   - Graphiques avec captions
   - Analyses statistiques
   - InterprÃ©tations
   - Tableaux clÃ©s

4. Recommandations (1-2 pages)
   - Pour la tarification
   - Pour la prÃ©vention
   - Pour la gestion des risques
   - Limitations et perspectives

5. Annexe technique
   - DÃ©tails nettoyage
   - Formules modÃ¨les
   - RÃ©fÃ©rences
```

---

## ğŸ’¡ Conseils de RÃ©daction

âœ… **ClartÃ©**: Chaque figure doit Ãªtre explicite (titre, axes, lÃ©gende)  
âœ… **Concision**: Aller Ã  l'essentiel, pas de redondance  
âœ… **InterprÃ©tation**: Toujours expliquer le "pourquoi"  
âœ… **Critique**: Souligner limitations et hypothÃ¨ses  
âœ… **Rigueur**: Tests stats, p-values, intervalle confiance  
âœ… **Actionnable**: Recommandations concrÃ¨tes pour business

---

## ğŸ“ Ã‰valuation (ce que les profs regardent)

| CritÃ¨re | Points | DÃ©tails |
|---------|--------|---------|
| Fond | 60% | Analyses pertinentes, interprÃ©tations, rigueur |
| Forme | 20% | ClartÃ©, organisation, prÃ©sentation |
| Technique | 15% | Python, modÃ¨les, visualisations |
| Innovation | 5% | Approches crÃ©atives, insights originales |

---

## ğŸ“š Ressources

- [scikit-learn documentation](https://scikit-learn.org)
- [Pandas for data analysis](https://pandas.pydata.org)
- [Seaborn visualization](https://seaborn.pydata.org)
- [Statistical tests with SciPy](https://docs.scipy.org/doc/scipy/)
- [INSEE donnÃ©es](https://www.insee.fr)
- [data.gouv.fr open data](https://www.data.gouv.fr)

---

**Bonne chance! Faites du code propre et des analyses pertinentes.** ğŸš€
