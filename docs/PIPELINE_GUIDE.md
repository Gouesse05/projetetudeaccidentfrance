# ğŸ“¥ Guide d'utilisation du Pipeline ETL

## Vue d'ensemble

Le pipeline ETL automatise:
1. **TÃ©lÃ©chargement** des donnÃ©es d'accidents depuis data.gouv.fr
2. **Exploration** complÃ¨te des datasets
3. **Nettoyage** intelligent des donnÃ©es
4. **PrÃ©paration** pour import PostgreSQL

## ğŸš€ DÃ©marrage rapide

### Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

### Exploration des sources

```bash
cd src/pipeline
python explore_datasources.py
```

Cela affiche:
- Les datasets disponibles sur data.gouv.fr
- Les organisations de SÃ©curitÃ© RoutiÃ¨re
- Les URLs des fichiers CSV

### TÃ©lÃ©charger les donnÃ©es

```bash
python download_data.py
```

**RÃ©sultat**: Fichiers tÃ©lÃ©chargÃ©s dans `data/raw/`

### Explorer et nettoyer

```bash
python explore_and_clean.py
```

**RÃ©sultat**: 
- Statistiques complÃ¨tes des donnÃ©es brutes
- Fichiers nettoyÃ©s dans `data/clean/`
- Rapport de qualitÃ©

### Pipeline complet (recommandÃ©)

```bash
python run_pipeline.py
```

Combine tout en une seule commande avec logging complet.

## ğŸ“Š Exemple d'exÃ©cution

```bash
$ python run_pipeline.py

================================================================================
ğŸš€ DÃ‰MARRAGE PIPELINE ETL - ACCIDENTS ROUTIERS
================================================================================
Heure: 2024-01-22 14:30:45

ğŸ“¥ Ã‰TAPE 1: TÃ‰LÃ‰CHARGEMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ” Dataset: accidents
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Ressource trouvÃ©e: accidents.csv
âœ“ 1 ressources trouvÃ©es pour accidents
ğŸ” Calcul du hash distant...
ğŸ“¥ TÃ©lÃ©chargement: accidents.csv
  Progression: 100.0%
âœ“ TÃ©lÃ©chargement rÃ©ussi: accidents.csv

... (autres datasets)

âœ“ 5/5 datasets tÃ©lÃ©chargÃ©s

ğŸ” Ã‰TAPE 2: EXPLORATION ET NETTOYAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“‚ Exploration: accidents.csv
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Lignes: 1,234,567
  Colonnes: 28
  Taille: 245.50 MB
  Doublons: 1,234

  ğŸ“‹ Colonnes:
    - Num_Acc                     | int64           | Missing:      0 ( 0.00%)
    - Date                        | object          | Missing:    125 ( 0.01%)
    - an                          | int64           | Missing:      0 ( 0.00%)
    ...

ğŸ§¹ Nettoyage donnÃ©es accidents
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Doublons supprimÃ©s: 1,234
  Noms de colonnes normalisÃ©s
  Valeurs manquantes traitÃ©es
âœ“ Nettoyage terminÃ©: 1,233,333 lignes

... (autres fichiers)

ğŸ“Š RAPPORT DE QUALITÃ‰ DES DONNÃ‰ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ accidents.csv
  Lignes: 1,233,333
  Colonnes: 27
  Taille: 244.23 MB
  Doublons: 0
  DonnÃ©es manquantes:
    - grav: 5,234 (0.4%)
    - agglo: 2,345 (0.2%)

...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… PIPELINE COMPLÃ‰TÃ‰ AVEC SUCCÃˆS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‚ DonnÃ©es nettoyÃ©es disponibles dans: data/clean/
ğŸ“‹ Logs disponibles dans: pipeline.log
```

## ğŸ”§ Options avancÃ©es

### Forcer le re-tÃ©lÃ©chargement

```bash
python download_data.py --force
```

Ignore les mÃ©tadonnÃ©es et retÃ©lÃ©charge tous les fichiers.

### Sauter l'Ã©tape de tÃ©lÃ©chargement

```bash
python run_pipeline.py --skip-download
```

Utile pour tester le nettoyage sur des donnÃ©es existantes.

## ğŸ“ Fichiers gÃ©nÃ©rÃ©s

### MÃ©tadonnÃ©es de tÃ©lÃ©chargement

`data/raw/.metadata.json`:
```json
{
  "files": {
    "accidents.csv": {
      "hash": "abc123def456...",
      "remote_hash": "abc123def456...",
      "download_url": "https://...",
      "downloaded_at": "2024-01-22T14:30:45.123456",
      "size": 256789012
    }
  }
}
```

### Logs du pipeline

`pipeline.log`:
```
2024-01-22 14:30:45 - INFO - ğŸš€ DÃ‰MARRAGE PIPELINE ETL - ACCIDENTS ROUTIERS
2024-01-22 14:30:46 - INFO - âœ“ RÃ©pertoire donnÃ©es: /path/to/data/raw
2024-01-22 14:30:47 - INFO - ğŸ” Dataset: accidents
...
```

## ğŸ¯ Configuration personnalisÃ©e

Ã‰diter `src/pipeline/data_config.py` pour personnaliser:

### URLs des datasets

```python
DATASETS_CONFIG = {
    "accidents_securite_routiere": {
        "api_url": "https://...",
        "dataset_slug": "base-de-donnees-accidents-de-la-circulation",
        "csv_files": ["accidents.csv", ...]
    }
}
```

### Nettoyage

```python
CLEANING_CONFIG = {
    "accidents": {
        "primary_key": ["Num_Acc"],
        "date_columns": ["Date", "jour"],
        "remove_duplicates": True,
        "handle_missing": {
            "threshold_pct": 50
        }
    }
}
```

### Validation

```python
VALIDATION_CONFIG = {
    "min_rows_per_file": {
        "accidents": 100
    },
    "required_columns": {
        "accidents": ["Num_Acc", "Date", "an"]
    }
}
```

## ğŸ” DÃ©pannage

### Erreur de connexion rÃ©seau

```
âœ— Erreur tÃ©lÃ©chargement: ('Connection aborted.', RemoteDisconnected(...))
```

**Solution**: VÃ©rifier la connexion et les URLs dans `data_config.py`

### Fichier dÃ©jÃ  tÃ©lÃ©chargÃ© (hash identique)

```
  â†’ Hash identique, pas de mise Ã  jour
âœ“ Fichier dÃ©jÃ  Ã  jour, pas de tÃ©lÃ©chargement
```

**C'est normal!** Ajouter `--force` pour forcer.

### Erreur d'encodage UTF-8

```
âœ— Erreur lors de l'exploration: 'utf-8' codec can't decode byte 0x...
```

**Solution**: Changer l'encodage dans `data_config.py`:
```python
"encoding": "latin-1"  # ou iso-8859-1
```

### Pas de fichiers trouvÃ©s

```
âš  Aucun fichier CSV trouvÃ© dans data/raw/
```

**Solution**: ExÃ©cuter d'abord `python download_data.py`

## ğŸ“Š Comprendre les rÃ©sultats

### Structure des donnÃ©es nettoyÃ©es

```
data/clean/
â”œâ”€â”€ clean_accidents.csv          # Accidents routiers
â”œâ”€â”€ clean_caracteristiques.csv   # CaractÃ©ristiques dÃ©taillÃ©es
â”œâ”€â”€ clean_lieux.csv              # Lieux (gÃ©olocalisation)
â”œâ”€â”€ clean_usagers.csv            # DonnÃ©es d'usagers
â””â”€â”€ clean_vehicules.csv          # DonnÃ©es de vÃ©hicules
```

### Format des colonnes nettoyÃ©es

- **Noms**: minuscules avec underscores (`num_accident` au lieu de `Num Acc`)
- **Types**: convertis automatiquement (dates â†’ datetime, nombres â†’ numeric)
- **Valeurs manquantes**: traitÃ©es intelligemment
- **Doublons**: supprimÃ©s
- **Espaces**: supprimÃ©s des valeurs texte

## ğŸ”„ Automatisation

### ExÃ©cution quotidienne (cron)

```bash
# Ã‰diter crontab
crontab -e

# Ajouter:
0 3 * * * cd /path/to/projetetudeapi && python src/pipeline/run_pipeline.py --skip-download
```

ExÃ©cute Ã  3h du matin chaque jour (exploration/nettoyage seulement)

### Avec tÃ©lÃ©chargement hebdomadaire

```bash
# Crontab
0 3 * * 1 cd /path/to/projetetudeapi && python src/pipeline/run_pipeline.py
```

ExÃ©cute le pipeline complet chaque lundi Ã  3h du matin

## âœ… Validation du pipeline

### Tests

```bash
pytest tests/test_pipeline.py -v
```

### VÃ©rifier les mÃ©tadonnÃ©es

```bash
cat data/raw/.metadata.json
```

### VÃ©rifier les fichiers nettoyÃ©s

```bash
ls -lh data/clean/
```

## ğŸ“ˆ Prochaines Ã©tapes

Une fois le pipeline exÃ©cutÃ©:

1. **VÃ©rifier les donnÃ©es**: `data/clean/*.csv` contient les fichiers nettoyÃ©s
2. **CrÃ©er le schÃ©ma PostgreSQL**: Voir `src/database/`
3. **Charger les donnÃ©es**: `python src/pipeline/load_postgresql.py`
4. **Lancer l'API**: `uvicorn src.api.main:app`

## ğŸ†˜ Besoin d'aide?

- Consulter les logs: `tail -f pipeline.log`
- VÃ©rifier la configuration: `src/pipeline/data_config.py`
- Lancer en mode verbeux: Les scripts affichent tous les dÃ©tails en INFO level

## ğŸ“š Ressources

- [data.gouv.fr - Accidents routiers](https://www.data.gouv.fr/)
- [SÃ©curitÃ© RoutiÃ¨re - Open data](https://www.data.gouv.fr/organizations/securite-routiere/)
- [Pandas documentation](https://pandas.pydata.org/docs/)
- [Python requests](https://requests.readthedocs.io/)
