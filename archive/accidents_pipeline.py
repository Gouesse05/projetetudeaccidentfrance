"""
DAG Airflow pour orchestrer le pipeline ETL Accidents Routiers
Phase 1: TÃ©lÃ©chargement, nettoyage, chargement donnÃ©es
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.decorators import apply_defaults
import sys
import os

# Ajouter le path du projet
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Importer les fonctions ETL
from src.pipeline.download_data import main as download_data
from src.pipeline.clean_data import main as clean_data
from src.database.load_postgresql import main as load_postgresql


# ============================================================================
# Configuration par dÃ©faut de la DAG
# ============================================================================

default_args = {
    'owner': 'data-engineer',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2024, 1, 1),
    'email': ['admin@accidents.local'],
    'email_on_failure': False,
    'email_on_retry': False,
}

# ============================================================================
# DÃ©finition de la DAG
# ============================================================================

dag = DAG(
    dag_id='accidents_etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL complet: TÃ©lÃ©charger â†’ Nettoyer â†’ Charger donnÃ©es accidents',
    schedule_interval='0 3 * * 1',  # Lundi 3h du matin (cron)
    catchup=False,
    tags=['accidents', 'etl', 'production'],
    doc_md=__doc__,
)


# ============================================================================
# TÃ‚CHES PYTHON - Ã‰tapes ETL
# ============================================================================

def task_download_data(**context):
    """
    TÃ©lÃ©charger les donnÃ©es depuis data.gouv.fr
    TÃ©lÃ©charge les fichiers CSV dans data/raw/
    """
    print("ğŸ”„ DÃ©marrage tÃ©lÃ©chargement donnÃ©es...")
    try:
        download_data()
        print("âœ… TÃ©lÃ©chargement rÃ©ussi")
        return {'status': 'success', 'stage': 'download'}
    except Exception as e:
        print(f"âŒ Erreur tÃ©lÃ©chargement: {str(e)}")
        raise


def task_clean_data(**context):
    """
    Nettoyer et normaliser les donnÃ©es
    Traite les fichiers CSV et les exporte en donnÃ©es propres
    """
    print("ğŸ§¹ DÃ©marrage nettoyage donnÃ©es...")
    try:
        clean_data()
        print("âœ… Nettoyage rÃ©ussi")
        return {'status': 'success', 'stage': 'clean'}
    except Exception as e:
        print(f"âŒ Erreur nettoyage: {str(e)}")
        raise


def task_load_postgresql(**context):
    """
    Charger les donnÃ©es nettoyÃ©es dans PostgreSQL
    CrÃ©e les tables et insÃ¨re les donnÃ©es
    """
    print("ğŸ“Š DÃ©marrage chargement PostgreSQL...")
    try:
        load_postgresql()
        print("âœ… Chargement rÃ©ussi")
        return {'status': 'success', 'stage': 'load'}
    except Exception as e:
        print(f"âŒ Erreur chargement: {str(e)}")
        raise


def task_validate_data(**context):
    """
    Valider que les donnÃ©es sont correctement chargÃ©es
    VÃ©rifier les counts et la qualitÃ©
    """
    import psycopg2
    from src.config import DB_CONFIG
    
    print("âœ”ï¸  Validation des donnÃ©es...")
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # RequÃªtes de validation
        cursor.execute("SELECT COUNT(*) FROM accidents;")
        count_accidents = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM usagers;")
        count_usagers = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM vehicules;")
        count_vehicules = cursor.fetchone()[0]
        
        cursor.close()
        conn.close()
        
        results = {
            'accidents': count_accidents,
            'usagers': count_usagers,
            'vehicules': count_vehicules,
        }
        
        print(f"ğŸ“ˆ Statistiques:")
        print(f"   Accidents: {count_accidents:,}")
        print(f"   Usagers: {count_usagers:,}")
        print(f"   VÃ©hicules: {count_vehicules:,}")
        
        if count_accidents > 0 and count_usagers > 0 and count_vehicules > 0:
            print("âœ… Validation rÃ©ussie")
            return results
        else:
            raise ValueError("DonnÃ©es insuffisantes aprÃ¨s chargement")
            
    except Exception as e:
        print(f"âŒ Erreur validation: {str(e)}")
        raise


# ============================================================================
# TÃ‚CHES BASH - TÃ¢ches systÃ¨me
# ============================================================================

task_start = BashOperator(
    task_id='start_pipeline',
    bash_command='echo "ğŸš€ DÃ©marrage du pipeline ETL Ã  $(date)"',
    dag=dag,
)

task_verify_dirs = BashOperator(
    task_id='verify_directories',
    bash_command='''
    echo "VÃ©rification des rÃ©pertoires..."
    [ -d "/home/sdd/projetetudeapi/data/raw" ] && echo "âœ“ data/raw existe" || mkdir -p /home/sdd/projetetudeapi/data/raw
    [ -d "/home/sdd/projetetudeapi/data/clean" ] && echo "âœ“ data/clean existe" || mkdir -p /home/sdd/projetetudeapi/data/clean
    ls -lh /home/sdd/projetetudeapi/data/
    ''',
    dag=dag,
)

task_cleanup_temp = BashOperator(
    task_id='cleanup_temp_files',
    bash_command='find /tmp -name "accidents*" -type f -mtime +7 -delete 2>/dev/null || true',
    dag=dag,
)

task_end = BashOperator(
    task_id='end_pipeline',
    bash_command='echo "âœ… Pipeline terminÃ© Ã  $(date)"',
    dag=dag,
)


# ============================================================================
# OPÃ‰RATEURS PYTHON - TÃ¢ches ETL
# ============================================================================

download_op = PythonOperator(
    task_id='download_data',
    python_callable=task_download_data,
    provide_context=True,
    dag=dag,
)

clean_op = PythonOperator(
    task_id='clean_data',
    python_callable=task_clean_data,
    provide_context=True,
    dag=dag,
)

load_op = PythonOperator(
    task_id='load_postgresql',
    python_callable=task_load_postgresql,
    provide_context=True,
    dag=dag,
)

validate_op = PythonOperator(
    task_id='validate_data',
    python_callable=task_validate_data,
    provide_context=True,
    dag=dag,
)


# ============================================================================
# DÃ‰PENDANCES - Ordre d'exÃ©cution
# ============================================================================

# Flux principal
task_start >> task_verify_dirs >> download_op >> clean_op >> load_op >> validate_op >> task_cleanup_temp >> task_end

# Ordre:
# 1. DÃ©marrer
# 2. VÃ©rifier rÃ©pertoires
# 3. TÃ©lÃ©charger donnÃ©es (data.gouv.fr â†’ data/raw/)
# 4. Nettoyer donnÃ©es (data/raw/ â†’ data/clean/)
# 5. Charger dans PostgreSQL (data/clean/ â†’ PostgreSQL)
# 6. Valider les donnÃ©es chargÃ©es
# 7. Nettoyer fichiers temporaires
# 8. Terminer

if __name__ == "__main__":
    print("DAG 'accidents_etl_pipeline' dÃ©finie avec succÃ¨s!")
    print("Utilise: airflow dags list")
    print("Ou: airflow dags test accidents_etl_pipeline 2024-01-01")
