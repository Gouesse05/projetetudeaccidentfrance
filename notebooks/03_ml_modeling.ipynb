{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85549ae",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import requests\n",
    "\n",
    "# Modules du projet\n",
    "from analyses.machine_learning import MachineLearningAnalysis\n",
    "from analyses.data_cleaning import DataCleaning\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "API_BASE_URL = \"http://localhost:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399dc12",
   "metadata": {},
   "source": [
    "## 2. Chargement et Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd88c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Charge et prépare les données pour le ML\"\"\"\n",
    "    try:\n",
    "        # Charger via API\n",
    "        response = requests.get(f\"{API_BASE_URL}/accidents?limit=10000\")\n",
    "        df = pd.DataFrame(response.json())\n",
    "        print(f\"Données chargées via API: {len(df)} accidents\")\n",
    "    except:\n",
    "        # Fallback: CSV\n",
    "        df = pd.read_csv('../data/clean/accidents_clean.csv')\n",
    "        print(f\"Données chargées depuis CSV: {len(df)} accidents\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_and_prepare_data()\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382052f6",
   "metadata": {},
   "source": [
    "## 3. Définition de la Variable Cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aff728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable cible: gravité binaire (grave vs non-grave)\n",
    "if 'grav' in df.columns:\n",
    "    df['is_grave'] = (df['grav'] >= 3).astype(int)  # 3=Hospitalisé, 4=Tué\n",
    "    target = 'is_grave'\n",
    "    \n",
    "    print(\"Distribution de la cible:\")\n",
    "    print(df[target].value_counts())\n",
    "    print(f\"\\nProportion d'accidents graves: {df[target].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df[target].value_counts().plot(kind='bar', color=['steelblue', 'darkred'])\n",
    "    plt.title('Distribution de la Variable Cible')\n",
    "    plt.xlabel('Accident Grave (0=Non, 1=Oui)')\n",
    "    plt.ylabel('Nombre d\\'accidents')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[ATTENTION] Colonne 'grav' non trouvée\")\n",
    "    target = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bba68",
   "metadata": {},
   "source": [
    "## 4. Sélection des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406dc6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features candidates (colonnes numériques)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features = [col for col in numeric_cols if col not in ['grav', 'is_grave', 'Num_Acc']]\n",
    "\n",
    "print(f\"Features sélectionnées ({len(features)}):\")\n",
    "for i, feat in enumerate(features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Préparer X et y\n",
    "X = df[features].fillna(0)  # Remplir les NaN\n",
    "y = df[target]\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e537a4f",
   "metadata": {},
   "source": [
    "## 5. Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split stratifié\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SPLIT TRAIN/TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nDistribution dans train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Distribution dans test: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44b73c",
   "metadata": {},
   "source": [
    "## 6. Entraînement du Modèle Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le module ML\n",
    "ml = MachineLearningAnalysis(df)\n",
    "\n",
    "# Entraîner Random Forest\n",
    "print(\"Entraînement du modèle Random Forest...\\n\")\n",
    "results = ml.random_forest_classifier(\n",
    "    target_column=target,\n",
    "    feature_columns=features,\n",
    "    n_estimators=100,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RÉSULTATS DU MODÈLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"\\nRapport de classification:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c8614",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser l'importance des features\n",
    "feature_importance = results['feature_importance']\n",
    "top_features = feature_importance.nlargest(10)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP 10 FEATURES IMPORTANTES\")\n",
    "print(\"=\" * 60)\n",
    "for feature, importance in top_features.items():\n",
    "    print(f\"{feature:20s}: {importance:.4f}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features.sort_values().plot(kind='barh', color='steelblue')\n",
    "plt.title('Top 10 Features les Plus Importantes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f982d0",
   "metadata": {},
   "source": [
    "## 8. Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = results['confusion_matrix']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non Grave', 'Grave'],\n",
    "            yticklabels=['Non Grave', 'Grave'])\n",
    "plt.title('Matrice de Confusion', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe Prédite')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcul des métriques depuis la matrice\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"MÉTRIQUES DÉTAILLÉES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True Negatives:  {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives:  {tp}\")\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ecc25",
   "metadata": {},
   "source": [
    "## 9. Prédictions sur Nouveaux Cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a59ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de prédiction sur données de test\n",
    "model = results['model']\n",
    "\n",
    "# Prendre 5 exemples du test set\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "X_sample = X_test.iloc[sample_indices]\n",
    "y_sample = y_test.iloc[sample_indices]\n",
    "\n",
    "# Prédictions\n",
    "predictions = model.predict(X_sample)\n",
    "probas = model.predict_proba(X_sample)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXEMPLES DE PRÉDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (pred, true, proba) in enumerate(zip(predictions, y_sample, probas), 1):\n",
    "    status = \"[CORRECT]\" if pred == true else \"[INCORRECT]\"\n",
    "    print(f\"\\nExemple {i}:\")\n",
    "    print(f\"  Prédiction: {'Grave' if pred == 1 else 'Non Grave'} (confiance: {proba[pred]*100:.1f}%)\")\n",
    "    print(f\"  Réalité:    {'Grave' if true == 1 else 'Non Grave'}\")\n",
    "    print(f\"  Résultat:   {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbcf991",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f797479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Créer dossier models s'il n'existe pas\n",
    "models_dir = Path('../data/models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model_path = models_dir / 'random_forest_gravity.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Modèle sauvegardé: {model_path}\")\n",
    "\n",
    "# Sauvegarder les features utilisées\n",
    "features_path = models_dir / 'features.txt'\n",
    "with open(features_path, 'w') as f:\n",
    "    f.write('\\n'.join(features))\n",
    "print(f\"Features sauvegardées: {features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00ad91",
   "metadata": {},
   "source": [
    "## 11. Résumé Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0001765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RÉSUMÉ DU MODÈLE ML\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Modèle: Random Forest Classifier\")\n",
    "print(f\"Features: {len(features)}\")\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"\\nTop 3 features:\")\n",
    "for i, (feat, imp) in enumerate(top_features.head(3).items(), 1):\n",
    "    print(f\"  {i}. {feat}: {imp:.4f}\")\n",
    "print(\"\\nModélisation ML terminée!\")\n",
    "print(\"\\nProchain notebook: 04_visualizations.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
